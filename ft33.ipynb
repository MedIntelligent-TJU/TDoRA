{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import os  \n",
    "import shutil  \n",
    "import glob  \n",
    "import random  \n",
    "from datetime import datetime  \n",
    "from os.path import join, basename  \n",
    "\n",
    "from tqdm import tqdm  \n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F  \n",
    "from torch.utils.data import Dataset, DataLoader  \n",
    "import monai  \n",
    "from segment_anything import sam_model_registry  \n",
    "\n",
    "import argparse  \n",
    "random.seed(2023)  \n",
    "torch.cuda.empty_cache()  \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"  \n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"  \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"6\"  \n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"  \n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from os.path import basename\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data_root, bbox_shift=20):\n",
    "        self.data_root = data_root\n",
    "        self.gt_path = os.path.join(data_root, \"gts\")\n",
    "        self.img_path = os.path.join(data_root, \"imgs\")\n",
    "        self.gt_path_files = sorted(\n",
    "            glob.glob(os.path.join(self.gt_path, \"**\", \"*.npy\"), recursive=True)\n",
    "        )\n",
    "        self.gt_path_files = [\n",
    "            file for file in self.gt_path_files if os.path.isfile(os.path.join(self.img_path, basename(file)))\n",
    "        ]\n",
    "        self.bbox_shift = bbox_shift\n",
    "        print(f\"number of images: {len(self.gt_path_files)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.gt_path_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = basename(self.gt_path_files[index])\n",
    "        img_file_path = os.path.join(self.img_path, img_name)\n",
    "\n",
    "        if not os.path.isfile(img_file_path):\n",
    "            raise FileNotFoundError(f\"Image file not found: {img_file_path}\")\n",
    "\n",
    "        img_1024 = np.load(img_file_path, allow_pickle=True)\n",
    "        img_1024 = np.transpose(img_1024, (2, 0, 1))\n",
    "\n",
    "        gt_file_path = self.gt_path_files[index]\n",
    "        gt = np.load(gt_file_path, allow_pickle=True)\n",
    "\n",
    "        label_ids = np.unique(gt)\n",
    "        if len(label_ids) <= 1:\n",
    "            raise ValueError(f\"Ground truth contains insufficient label variations in file: {gt_file_path}\")\n",
    "\n",
    "        label_ids = label_ids[1:]  # Exclude background (assuming 0 is background)\n",
    "        selected_label = random.choice(label_ids.tolist())\n",
    "\n",
    "        gt2D = np.uint8(gt == selected_label)\n",
    "        y_indices, x_indices = np.where(gt2D > 0)\n",
    "\n",
    "        if len(x_indices) == 0 or len(y_indices) == 0:\n",
    "            raise ValueError(f\"No positive pixels found in ground truth for file: {gt_file_path}\")\n",
    "\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        H, W = gt2D.shape\n",
    "\n",
    "        x_min = max(0, x_min - random.randint(0, self.bbox_shift))\n",
    "        x_max = min(W, x_max + random.randint(0, self.bbox_shift))\n",
    "        y_min = max(0, y_min - random.randint(0, self.bbox_shift))\n",
    "        y_max = min(H, y_max + random.randint(0, self.bbox_shift))\n",
    "\n",
    "        bboxes = np.array([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        return (\n",
    "            torch.tensor(img_1024).float(),\n",
    "            torch.tensor(gt2D[None, :, :]).long(),\n",
    "            torch.tensor(bboxes).float(),\n",
    "            img_name\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset with configuration setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task_name = \"MedSAM-ViT-B\"  \n",
    "model_type = \"vit_b\"  \n",
    "checkpoint = \"/home/hzhb/Karry/medsam/work_dir/SAM/sam_vit_b_01ec64.pth\"  \n",
    "load_pretrain = Truepretrain_model_path = \"\"  \n",
    "work_dir = \"./work_dir\"  \n",
    "num_epochs = 500\n",
    "batch_size = 2\n",
    "num_workers = 0\n",
    "weight_decay = 0.01\n",
    "lr = 0.001\n",
    "use_wandb = False\n",
    "use_amp = False\n",
    "resume = \"\"\n",
    "\n",
    "device = \"cuda:0\"  \n",
    "\n",
    "if use_wandb:  \n",
    " import wandb \n",
    " wandb.login()  \n",
    " wandb.init(  \n",
    " project=task_name,  \n",
    " config={  \n",
    " \"lr\": lr,  \n",
    " \"batch_size\": batch_size,  \n",
    " \"data_path\": tr_npy_path,  \n",
    " \"model_type\": model_type,  \n",
    " },  \n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Initialization and Forward Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, linear_layer, m_tensor, v_tensor, block_idx, A, B, C, D, E):  \n",
    "    super().__init__()  \n",
    "    self.linear = linear_layer  \n",
    "    self.m = m_tensor[block_idx]  \n",
    "    # Frozen m with shape (3 * embed_dim)  \n",
    "    self.v = v_tensor[block_idx]  \n",
    "    # Frozen v with shape (3 * embed_dim, embed_dim)  \n",
    "    self.A = A[block_idx]  \n",
    "    # Trainable A with shape (3 * embed_dim)  \n",
    "    self.B = B[block_idx]  \n",
    "    # Trainable B with shape (3 * embed_dim, embed_dim)  \n",
    "    self.C = C  # Trainable C with shape (embed_dim, embed_dim)  \n",
    "    self.D = D  # Trainable D with shape (embed_dim, embed_dim)  \n",
    "    self.E = E  # Trainable E with shape (embed_dim, embed_dim)  \n",
    "    self.bias = self.linear.bias  \n",
    "    self.linear.weight = None  # Remove original weight param\n",
    "\n",
    "def forward(self, x):  \n",
    "    m_new = self.m + self.A  \n",
    "    B_transformed = torch.matmul(self.B, self.C)  \n",
    "    B_transformed = torch.matmul(B_transformed, self.D)  \n",
    "    B_transformed = torch.matmul(B_transformed, self.E)  \n",
    "    v_new = self.v + B_transformed  \n",
    "    weight = m_new.unsqueeze(1) * v_new  \n",
    "    return F.linear(x, weight, self.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyDataset DataLoader Implementation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 20; Testing samples: 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define your dataset class\n",
    "class NpyDataset(Dataset):\n",
    "    def __init__(self, img_dir, gt_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.npy')]\n",
    "        self.gt_files = [f for f in os.listdir(gt_dir) if f.endswith('.npy')]\n",
    "        self.img_files.sort()  # Ensure consistent ordering\n",
    "        self.gt_files.sort()   # Ensure consistent ordering\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset based on the number of images\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
    "        gt_path = os.path.join(self.gt_dir, self.gt_files[idx])\n",
    "        \n",
    "        # Load image and ground truth mask\n",
    "        image = np.load(img_path)\n",
    "        mask = np.load(gt_path)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define paths and parameters\n",
    "tr_npy_path = \"/home/hzhb/Karry/data/眼底npy\"  # Change to your training data path\n",
    "train_img_path = os.path.join(tr_npy_path, \"imgs\")\n",
    "train_gt_path = os.path.join(tr_npy_path, \"gts\")\n",
    "batch_size = 10  # Define your desired batch size\n",
    "num_workers = 4  # Adjust based on your system\n",
    "\n",
    "# Initialize the train dataset\n",
    "train_dataset = NpyDataset(train_img_path, train_gt_path)\n",
    "\n",
    "# Create DataLoader for training dataset\n",
    "train_dataloader = DataLoader(  \n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")  \n",
    "\n",
    "# Test Dataset \n",
    "test_npy_path = os.path.join(\"/home/hzhb/Karry/data\", \"眼底npy_test\")\n",
    "test_img_path = os.path.join(test_npy_path, \"imgs\")\n",
    "test_gt_path = os.path.join(test_npy_path, \"gts\")\n",
    "test_dataset = NpyDataset(test_img_path, test_gt_path)\n",
    "\n",
    "test_dataloader = DataLoader(  \n",
    "    test_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")  \n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}; Testing samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the imagee shape and mask shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (1024, 1024, 3), Mask shape: (1024, 1024)\n",
      "Image shape: (1024, 1024, 3), Mask shape: (1024, 1024)\n",
      "Image shape: (1024, 1024, 3), Mask shape: (1024, 1024)\n",
      "Image shape: (1024, 1024, 3), Mask shape: (1024, 1024)\n",
      "Image shape: (1024, 1024, 3), Mask shape: (1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Example of checking a few training samples\n",
    "for i in range(5):  # Print first 5 samples\n",
    "    img, mask = train_dataset[i]\n",
    "    print(f\"Image shape: {img.shape}, Mask shape: {mask.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FCN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FCN, self).__init__()\n",
    "        \n",
    "        # Encoder (VGG-like structure)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        # Final convolution layer\n",
    "        self.final_conv = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
    "        x = self.pool(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
    "        x = self.pool(F.relu(self.conv7(F.relu(self.conv6(F.relu(self.conv5(x)))))))\n",
    "        x = self.pool(F.relu(self.conv10(F.relu(self.conv9(F.relu(self.conv8(x)))))))\n",
    "        \n",
    "        # Final convolution\n",
    "        x = self.final_conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpyDataset(Dataset):\n",
    "    def __init__(self, npy_path):\n",
    "        self.npy_path = npy_path\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        data_files = [f for f in os.listdir(self.npy_path) if f.endswith('.npy')]\n",
    "        loaded_data = []\n",
    "        for file in data_files:\n",
    "            file_path = os.path.join(self.npy_path, file)\n",
    "            loaded_data.append(np.load(file_path))\n",
    "        return loaded_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx][0]  # Assuming the image is at index 0\n",
    "        mask = self.data[idx][1]   # Assuming the mask is at index 1\n",
    "        \n",
    "        # Ensure image is float32 and mask is long (for cross-entropy loss)\n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)  # Change to CxHxW\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        \n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming 'images' is a torch tensor with shape [10, 1024, 1024, 3]\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mimages\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Permute to get [batch, channels, height, width]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# # Assuming 'images' is a torch tensor with shape [10, 1024, 1024, 3]\n",
    "# print(\"Original shape:\", images.shape)\n",
    "\n",
    "# # Permute to get [batch, channels, height, width]\n",
    "# images = images.permute(0, 3, 1, 2)\n",
    "\n",
    "# # Verify the final shape\n",
    "# print(\"Final shape:\", images.shape)  # Expected shape: [10, 3, 1024, 1024]\n",
    "\n",
    "# # Now pass the images to the model\n",
    "# outputs = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now the shape is correct: [20, 3, 1024, 1024]\n",
    "# outputs = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        # Define your model architecture here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass here\n",
    "        return x  # Modify as per your architecture\n",
    "\n",
    "# Instantiate the model\n",
    "model = FCN().to(device)  # Ensure you're moving the model to the appropriate device (GPU/CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Class Defination   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MyDataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_root, bbox_shift=20):\n",
    "        self.data_root = data_root\n",
    "        self.gt_path = os.path.join(data_root, \"gts\")\n",
    "        self.img_path = os.path.join(data_root, \"imgs\")\n",
    "        self.gt_path_files = sorted(\n",
    "            glob.glob(os.path.join(self.gt_path, \"**\", \"*.npy\"), recursive=True)\n",
    "        )\n",
    "        self.gt_path_files = [\n",
    "            file for file in self.gt_path_files if os.path.isfile(os.path.join(self.img_path, os.path.basename(file)))\n",
    "        ]\n",
    "        self.bbox_shift = bbox_shift\n",
    "        print(f\"Number of images: {len(self.gt_path_files)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.gt_path_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = os.path.basename(self.gt_path_files[index])\n",
    "        img_file_path = os.path.join(self.img_path, img_name)\n",
    "\n",
    "        if not os.path.isfile(img_file_path):\n",
    "            raise FileNotFoundError(f\"Image file not found: {img_file_path}\")\n",
    "\n",
    "        img_1024 = np.load(img_file_path, allow_pickle=True)\n",
    "        img_1024 = np.transpose(img_1024, (2, 0, 1))  # Change dimensions to CxHxW\n",
    "\n",
    "        gt_file_path = self.gt_path_files[index]\n",
    "        gt = np.load(gt_file_path, allow_pickle=True)\n",
    "\n",
    "        label_ids = np.unique(gt)\n",
    "        if len(label_ids) <= 1:\n",
    "            raise ValueError(f\"Ground truth contains insufficient label variations in file: {gt_file_path}\")\n",
    "\n",
    "        label_ids = label_ids[1:]  # Exclude background (assuming 0 is background)\n",
    "        selected_label = np.random.choice(label_ids.tolist())\n",
    "\n",
    "        gt2D = np.uint8(gt == selected_label)\n",
    "        y_indices, x_indices = np.where(gt2D > 0)\n",
    "\n",
    "        if len(x_indices) == 0 or len(y_indices) == 0:\n",
    "            raise ValueError(f\"No positive pixels found in ground truth for file: {gt_file_path}\")\n",
    "\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        H, W = gt2D.shape\n",
    "\n",
    "        x_min = max(0, x_min - np.random.randint(0, self.bbox_shift))\n",
    "        x_max = min(W, x_max + np.random.randint(0, self.bbox_shift))\n",
    "        y_min = max(0, y_min - np.random.randint(0, self.bbox_shift))\n",
    "        y_max = min(H, y_max + np.random.randint(0, self.bbox_shift))\n",
    "\n",
    "        bboxes = np.array([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        return (\n",
    "            torch.tensor(img_1024).float(),\n",
    "            torch.tensor(gt2D[None, :, :]).long(),\n",
    "            torch.tensor(bboxes).float(),\n",
    "            img_name\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FCN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.conv3(x)  # Shape: (batch_size, num_classes, height/4, width/4)\n",
    "        \n",
    "        # Upsample to the original size\n",
    "        x = nn.functional.interpolate(x, size=(1024, 1024), mode='bilinear', align_corners=False)  # Change size accordingly\n",
    "        return x  # Shape: (batch_size, num_classes, height, width)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FCN model with required arguments\n",
    "num_classes = 2  # Set this according to your number of classes\n",
    "model = FCN(num_classes)  # Pass the num_classes parameter\n",
    "model = model.to(device)  # Move model to GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data laoder setup (defines the loss function and the optimizer used for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataLoader Setup (sets up the DataLoader for loading data in batches)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 20\n"
     ]
    }
   ],
   "source": [
    "# Set up DataLoader\n",
    "train_dataset = MyDataset(data_root='/home/hzhb/Karry/data/眼底npy')\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=2,        # Your desired batch size\n",
    "    shuffle=True,        # Shuffle the data\n",
    "    num_workers=0        # Number of CPU cores to load data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop Succesful run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 0.4731\n",
      "Epoch [2/500], Loss: 0.3551\n",
      "Epoch [3/500], Loss: 0.3087\n",
      "Epoch [4/500], Loss: 0.2821\n",
      "Epoch [5/500], Loss: 0.2707\n",
      "Epoch [6/500], Loss: 0.2643\n",
      "Epoch [7/500], Loss: 0.2624\n",
      "Epoch [8/500], Loss: 0.2604\n",
      "Epoch [9/500], Loss: 0.2591\n",
      "Epoch [10/500], Loss: 0.2597\n",
      "Epoch [11/500], Loss: 0.2587\n",
      "Epoch [12/500], Loss: 0.2584\n",
      "Epoch [13/500], Loss: 0.2577\n",
      "Epoch [14/500], Loss: 0.2577\n",
      "Epoch [15/500], Loss: 0.2579\n",
      "Epoch [16/500], Loss: 0.2568\n",
      "Epoch [17/500], Loss: 0.2563\n",
      "Epoch [18/500], Loss: 0.2562\n",
      "Epoch [19/500], Loss: 0.2560\n",
      "Epoch [20/500], Loss: 0.2562\n",
      "Epoch [21/500], Loss: 0.2555\n",
      "Epoch [22/500], Loss: 0.2557\n",
      "Epoch [23/500], Loss: 0.2558\n",
      "Epoch [24/500], Loss: 0.2564\n",
      "Epoch [25/500], Loss: 0.2577\n",
      "Epoch [26/500], Loss: 0.2555\n",
      "Epoch [27/500], Loss: 0.2557\n",
      "Epoch [28/500], Loss: 0.2551\n",
      "Epoch [29/500], Loss: 0.2559\n",
      "Epoch [30/500], Loss: 0.2549\n",
      "Epoch [31/500], Loss: 0.2540\n",
      "Epoch [32/500], Loss: 0.2539\n",
      "Epoch [33/500], Loss: 0.2541\n",
      "Epoch [34/500], Loss: 0.2540\n",
      "Epoch [35/500], Loss: 0.2576\n",
      "Epoch [36/500], Loss: 0.2565\n",
      "Epoch [37/500], Loss: 0.2555\n",
      "Epoch [38/500], Loss: 0.2536\n",
      "Epoch [39/500], Loss: 0.2541\n",
      "Epoch [40/500], Loss: 0.2546\n",
      "Epoch [41/500], Loss: 0.2534\n",
      "Epoch [42/500], Loss: 0.2533\n",
      "Epoch [43/500], Loss: 0.2547\n",
      "Epoch [44/500], Loss: 0.2533\n",
      "Epoch [45/500], Loss: 0.2526\n",
      "Epoch [46/500], Loss: 0.2564\n",
      "Epoch [47/500], Loss: 0.2546\n",
      "Epoch [48/500], Loss: 0.2516\n",
      "Epoch [49/500], Loss: 0.2537\n",
      "Epoch [50/500], Loss: 0.2539\n",
      "Epoch [51/500], Loss: 0.2537\n",
      "Epoch [52/500], Loss: 0.2523\n",
      "Epoch [53/500], Loss: 0.2522\n",
      "Epoch [54/500], Loss: 0.2521\n",
      "Epoch [55/500], Loss: 0.2512\n",
      "Epoch [56/500], Loss: 0.2511\n",
      "Epoch [57/500], Loss: 0.2529\n",
      "Epoch [58/500], Loss: 0.2524\n",
      "Epoch [59/500], Loss: 0.2514\n",
      "Epoch [60/500], Loss: 0.2509\n",
      "Epoch [61/500], Loss: 0.2504\n",
      "Epoch [62/500], Loss: 0.2548\n",
      "Epoch [63/500], Loss: 0.2524\n",
      "Epoch [64/500], Loss: 0.2521\n",
      "Epoch [65/500], Loss: 0.2504\n",
      "Epoch [66/500], Loss: 0.2509\n",
      "Epoch [67/500], Loss: 0.2517\n",
      "Epoch [68/500], Loss: 0.2500\n",
      "Epoch [69/500], Loss: 0.2510\n",
      "Epoch [70/500], Loss: 0.2499\n",
      "Epoch [71/500], Loss: 0.2493\n",
      "Epoch [72/500], Loss: 0.2554\n",
      "Epoch [73/500], Loss: 0.2527\n",
      "Epoch [74/500], Loss: 0.2516\n",
      "Epoch [75/500], Loss: 0.2532\n",
      "Epoch [76/500], Loss: 0.2527\n",
      "Epoch [77/500], Loss: 0.2533\n",
      "Epoch [78/500], Loss: 0.2491\n",
      "Epoch [79/500], Loss: 0.2489\n",
      "Epoch [80/500], Loss: 0.2483\n",
      "Epoch [81/500], Loss: 0.2473\n",
      "Epoch [82/500], Loss: 0.2478\n",
      "Epoch [83/500], Loss: 0.2482\n",
      "Epoch [84/500], Loss: 0.2481\n",
      "Epoch [85/500], Loss: 0.2472\n",
      "Epoch [86/500], Loss: 0.2485\n",
      "Epoch [87/500], Loss: 0.2478\n",
      "Epoch [88/500], Loss: 0.2480\n",
      "Epoch [89/500], Loss: 0.2472\n",
      "Epoch [90/500], Loss: 0.2465\n",
      "Epoch [91/500], Loss: 0.2482\n",
      "Epoch [92/500], Loss: 0.2488\n",
      "Epoch [93/500], Loss: 0.2501\n",
      "Epoch [94/500], Loss: 0.2450\n",
      "Epoch [95/500], Loss: 0.2506\n",
      "Epoch [96/500], Loss: 0.2577\n",
      "Epoch [97/500], Loss: 0.2545\n",
      "Epoch [98/500], Loss: 0.2515\n",
      "Epoch [99/500], Loss: 0.2489\n",
      "Epoch [100/500], Loss: 0.2496\n",
      "Epoch [101/500], Loss: 0.2516\n",
      "Epoch [102/500], Loss: 0.2520\n",
      "Epoch [103/500], Loss: 0.2534\n",
      "Epoch [104/500], Loss: 0.2470\n",
      "Epoch [105/500], Loss: 0.2456\n",
      "Epoch [106/500], Loss: 0.2478\n",
      "Epoch [107/500], Loss: 0.2516\n",
      "Epoch [108/500], Loss: 0.2594\n",
      "Epoch [109/500], Loss: 0.2545\n",
      "Epoch [110/500], Loss: 0.2502\n",
      "Epoch [111/500], Loss: 0.2487\n",
      "Epoch [112/500], Loss: 0.2477\n",
      "Epoch [113/500], Loss: 0.2495\n",
      "Epoch [114/500], Loss: 0.2528\n",
      "Epoch [115/500], Loss: 0.2509\n",
      "Epoch [116/500], Loss: 0.2481\n",
      "Epoch [117/500], Loss: 0.2485\n",
      "Epoch [118/500], Loss: 0.2514\n",
      "Epoch [119/500], Loss: 0.2494\n",
      "Epoch [120/500], Loss: 0.2461\n",
      "Epoch [121/500], Loss: 0.2481\n",
      "Epoch [122/500], Loss: 0.2487\n",
      "Epoch [123/500], Loss: 0.2517\n",
      "Epoch [124/500], Loss: 0.2503\n",
      "Epoch [125/500], Loss: 0.2464\n",
      "Epoch [126/500], Loss: 0.2478\n",
      "Epoch [127/500], Loss: 0.2485\n",
      "Epoch [128/500], Loss: 0.2459\n",
      "Epoch [129/500], Loss: 0.2461\n",
      "Epoch [130/500], Loss: 0.2455\n",
      "Epoch [131/500], Loss: 0.2486\n",
      "Epoch [132/500], Loss: 0.2481\n",
      "Epoch [133/500], Loss: 0.2440\n",
      "Epoch [134/500], Loss: 0.2454\n",
      "Epoch [135/500], Loss: 0.2466\n",
      "Epoch [136/500], Loss: 0.2450\n",
      "Epoch [137/500], Loss: 0.2451\n",
      "Epoch [138/500], Loss: 0.2455\n",
      "Epoch [139/500], Loss: 0.2455\n",
      "Epoch [140/500], Loss: 0.2472\n",
      "Epoch [141/500], Loss: 0.2493\n",
      "Epoch [142/500], Loss: 0.2455\n",
      "Epoch [143/500], Loss: 0.2450\n",
      "Epoch [144/500], Loss: 0.2430\n",
      "Epoch [145/500], Loss: 0.2423\n",
      "Epoch [146/500], Loss: 0.2525\n",
      "Epoch [147/500], Loss: 0.2497\n",
      "Epoch [148/500], Loss: 0.2551\n",
      "Epoch [149/500], Loss: 0.2546\n",
      "Epoch [150/500], Loss: 0.2501\n",
      "Epoch [151/500], Loss: 0.2479\n",
      "Epoch [152/500], Loss: 0.2485\n",
      "Epoch [153/500], Loss: 0.2494\n",
      "Epoch [154/500], Loss: 0.2474\n",
      "Epoch [155/500], Loss: 0.2439\n",
      "Epoch [156/500], Loss: 0.2441\n",
      "Epoch [157/500], Loss: 0.2448\n",
      "Epoch [158/500], Loss: 0.2429\n",
      "Epoch [159/500], Loss: 0.2432\n",
      "Epoch [160/500], Loss: 0.2433\n",
      "Epoch [161/500], Loss: 0.2419\n",
      "Epoch [162/500], Loss: 0.2419\n",
      "Epoch [163/500], Loss: 0.2452\n",
      "Epoch [164/500], Loss: 0.2404\n",
      "Epoch [165/500], Loss: 0.2463\n",
      "Epoch [166/500], Loss: 0.2406\n",
      "Epoch [167/500], Loss: 0.2445\n",
      "Epoch [168/500], Loss: 0.2476\n",
      "Epoch [169/500], Loss: 0.2423\n",
      "Epoch [170/500], Loss: 0.2434\n",
      "Epoch [171/500], Loss: 0.2431\n",
      "Epoch [172/500], Loss: 0.2418\n",
      "Epoch [173/500], Loss: 0.2404\n",
      "Epoch [174/500], Loss: 0.2396\n",
      "Epoch [175/500], Loss: 0.2388\n",
      "Epoch [176/500], Loss: 0.2431\n",
      "Epoch [177/500], Loss: 0.2414\n",
      "Epoch [178/500], Loss: 0.2395\n",
      "Epoch [179/500], Loss: 0.2444\n",
      "Epoch [180/500], Loss: 0.2442\n",
      "Epoch [181/500], Loss: 0.2463\n",
      "Epoch [182/500], Loss: 0.2447\n",
      "Epoch [183/500], Loss: 0.2426\n",
      "Epoch [184/500], Loss: 0.2434\n",
      "Epoch [185/500], Loss: 0.2396\n",
      "Epoch [186/500], Loss: 0.2393\n",
      "Epoch [187/500], Loss: 0.2397\n",
      "Epoch [188/500], Loss: 0.2420\n",
      "Epoch [189/500], Loss: 0.2417\n",
      "Epoch [190/500], Loss: 0.2423\n",
      "Epoch [191/500], Loss: 0.2412\n",
      "Epoch [192/500], Loss: 0.2389\n",
      "Epoch [193/500], Loss: 0.2435\n",
      "Epoch [194/500], Loss: 0.2414\n",
      "Epoch [195/500], Loss: 0.2535\n",
      "Epoch [196/500], Loss: 0.2475\n",
      "Epoch [197/500], Loss: 0.2479\n",
      "Epoch [198/500], Loss: 0.2452\n",
      "Epoch [199/500], Loss: 0.2447\n",
      "Epoch [200/500], Loss: 0.2420\n",
      "Epoch [201/500], Loss: 0.2387\n",
      "Epoch [202/500], Loss: 0.2407\n",
      "Epoch [203/500], Loss: 0.2387\n",
      "Epoch [204/500], Loss: 0.2416\n",
      "Epoch [205/500], Loss: 0.2393\n",
      "Epoch [206/500], Loss: 0.2410\n",
      "Epoch [207/500], Loss: 0.2421\n",
      "Epoch [208/500], Loss: 0.2400\n",
      "Epoch [209/500], Loss: 0.2401\n",
      "Epoch [210/500], Loss: 0.2403\n",
      "Epoch [211/500], Loss: 0.2403\n",
      "Epoch [212/500], Loss: 0.2415\n",
      "Epoch [213/500], Loss: 0.2417\n",
      "Epoch [214/500], Loss: 0.2400\n",
      "Epoch [215/500], Loss: 0.2452\n",
      "Epoch [216/500], Loss: 0.2457\n",
      "Epoch [217/500], Loss: 0.2375\n",
      "Epoch [218/500], Loss: 0.2414\n",
      "Epoch [219/500], Loss: 0.2380\n",
      "Epoch [220/500], Loss: 0.2371\n",
      "Epoch [221/500], Loss: 0.2343\n",
      "Epoch [222/500], Loss: 0.2357\n",
      "Epoch [223/500], Loss: 0.2399\n",
      "Epoch [224/500], Loss: 0.2346\n",
      "Epoch [225/500], Loss: 0.2383\n",
      "Epoch [226/500], Loss: 0.2326\n",
      "Epoch [227/500], Loss: 0.2396\n",
      "Epoch [228/500], Loss: 0.2447\n",
      "Epoch [229/500], Loss: 0.2356\n",
      "Epoch [230/500], Loss: 0.2361\n",
      "Epoch [231/500], Loss: 0.2392\n",
      "Epoch [232/500], Loss: 0.2542\n",
      "Epoch [233/500], Loss: 0.2423\n",
      "Epoch [234/500], Loss: 0.2397\n",
      "Epoch [235/500], Loss: 0.2358\n",
      "Epoch [236/500], Loss: 0.2336\n",
      "Epoch [237/500], Loss: 0.2346\n",
      "Epoch [238/500], Loss: 0.2312\n",
      "Epoch [239/500], Loss: 0.2354\n",
      "Epoch [240/500], Loss: 0.2333\n",
      "Epoch [241/500], Loss: 0.2315\n",
      "Epoch [242/500], Loss: 0.2334\n",
      "Epoch [243/500], Loss: 0.2321\n",
      "Epoch [244/500], Loss: 0.2291\n",
      "Epoch [245/500], Loss: 0.2294\n",
      "Epoch [246/500], Loss: 0.2295\n",
      "Epoch [247/500], Loss: 0.2317\n",
      "Epoch [248/500], Loss: 0.2279\n",
      "Epoch [249/500], Loss: 0.2257\n",
      "Epoch [250/500], Loss: 0.2263\n",
      "Epoch [251/500], Loss: 0.2386\n",
      "Epoch [252/500], Loss: 0.2352\n",
      "Epoch [253/500], Loss: 0.2277\n",
      "Epoch [254/500], Loss: 0.2427\n",
      "Epoch [255/500], Loss: 0.2328\n",
      "Epoch [256/500], Loss: 0.2324\n",
      "Epoch [257/500], Loss: 0.2413\n",
      "Epoch [258/500], Loss: 0.2314\n",
      "Epoch [259/500], Loss: 0.2309\n",
      "Epoch [260/500], Loss: 0.2385\n",
      "Epoch [261/500], Loss: 0.2264\n",
      "Epoch [262/500], Loss: 0.2243\n",
      "Epoch [263/500], Loss: 0.2335\n",
      "Epoch [264/500], Loss: 0.2288\n",
      "Epoch [265/500], Loss: 0.2240\n",
      "Epoch [266/500], Loss: 0.2238\n",
      "Epoch [267/500], Loss: 0.2248\n",
      "Epoch [268/500], Loss: 0.2222\n",
      "Epoch [269/500], Loss: 0.2226\n",
      "Epoch [270/500], Loss: 0.2264\n",
      "Epoch [271/500], Loss: 0.2370\n",
      "Epoch [272/500], Loss: 0.2261\n",
      "Epoch [273/500], Loss: 0.2231\n",
      "Epoch [274/500], Loss: 0.2380\n",
      "Epoch [275/500], Loss: 0.2223\n",
      "Epoch [276/500], Loss: 0.2262\n",
      "Epoch [277/500], Loss: 0.2234\n",
      "Epoch [278/500], Loss: 0.2215\n",
      "Epoch [279/500], Loss: 0.2204\n",
      "Epoch [280/500], Loss: 0.2170\n",
      "Epoch [281/500], Loss: 0.2190\n",
      "Epoch [282/500], Loss: 0.2252\n",
      "Epoch [283/500], Loss: 0.2262\n",
      "Epoch [284/500], Loss: 0.2174\n",
      "Epoch [285/500], Loss: 0.2291\n",
      "Epoch [286/500], Loss: 0.2324\n",
      "Epoch [287/500], Loss: 0.2260\n",
      "Epoch [288/500], Loss: 0.2258\n",
      "Epoch [289/500], Loss: 0.2196\n",
      "Epoch [290/500], Loss: 0.2168\n",
      "Epoch [291/500], Loss: 0.2164\n",
      "Epoch [292/500], Loss: 0.2180\n",
      "Epoch [293/500], Loss: 0.2149\n",
      "Epoch [294/500], Loss: 0.2181\n",
      "Epoch [295/500], Loss: 0.2256\n",
      "Epoch [296/500], Loss: 0.2249\n",
      "Epoch [297/500], Loss: 0.2234\n",
      "Epoch [298/500], Loss: 0.2231\n",
      "Epoch [299/500], Loss: 0.2142\n",
      "Epoch [300/500], Loss: 0.2196\n",
      "Epoch [301/500], Loss: 0.2163\n",
      "Epoch [302/500], Loss: 0.2115\n",
      "Epoch [303/500], Loss: 0.2144\n",
      "Epoch [304/500], Loss: 0.2104\n",
      "Epoch [305/500], Loss: 0.2122\n",
      "Epoch [306/500], Loss: 0.2258\n",
      "Epoch [307/500], Loss: 0.2303\n",
      "Epoch [308/500], Loss: 0.2158\n",
      "Epoch [309/500], Loss: 0.2094\n",
      "Epoch [310/500], Loss: 0.2074\n",
      "Epoch [311/500], Loss: 0.2098\n",
      "Epoch [312/500], Loss: 0.2061\n",
      "Epoch [313/500], Loss: 0.2082\n",
      "Epoch [314/500], Loss: 0.2058\n",
      "Epoch [315/500], Loss: 0.2211\n",
      "Epoch [316/500], Loss: 0.2113\n",
      "Epoch [317/500], Loss: 0.2138\n",
      "Epoch [318/500], Loss: 0.2114\n",
      "Epoch [319/500], Loss: 0.2104\n",
      "Epoch [320/500], Loss: 0.2071\n",
      "Epoch [321/500], Loss: 0.2118\n",
      "Epoch [322/500], Loss: 0.2457\n",
      "Epoch [323/500], Loss: 0.2483\n",
      "Epoch [324/500], Loss: 0.2134\n",
      "Epoch [325/500], Loss: 0.2172\n",
      "Epoch [326/500], Loss: 0.2107\n",
      "Epoch [327/500], Loss: 0.2141\n",
      "Epoch [328/500], Loss: 0.2156\n",
      "Epoch [329/500], Loss: 0.2035\n",
      "Epoch [330/500], Loss: 0.2119\n",
      "Epoch [331/500], Loss: 0.2153\n",
      "Epoch [332/500], Loss: 0.2237\n",
      "Epoch [333/500], Loss: 0.2232\n",
      "Epoch [334/500], Loss: 0.2091\n",
      "Epoch [335/500], Loss: 0.2117\n",
      "Epoch [336/500], Loss: 0.2122\n",
      "Epoch [337/500], Loss: 0.2076\n",
      "Epoch [338/500], Loss: 0.2184\n",
      "Epoch [339/500], Loss: 0.2034\n",
      "Epoch [340/500], Loss: 0.2052\n",
      "Epoch [341/500], Loss: 0.2088\n",
      "Epoch [342/500], Loss: 0.2022\n",
      "Epoch [343/500], Loss: 0.2154\n",
      "Epoch [344/500], Loss: 0.2091\n",
      "Epoch [345/500], Loss: 0.2031\n",
      "Epoch [346/500], Loss: 0.2010\n",
      "Epoch [347/500], Loss: 0.2021\n",
      "Epoch [348/500], Loss: 0.1993\n",
      "Epoch [349/500], Loss: 0.1997\n",
      "Epoch [350/500], Loss: 0.2028\n",
      "Epoch [351/500], Loss: 0.2122\n",
      "Epoch [352/500], Loss: 0.2060\n",
      "Epoch [353/500], Loss: 0.2060\n",
      "Epoch [354/500], Loss: 0.1998\n",
      "Epoch [355/500], Loss: 0.1976\n",
      "Epoch [356/500], Loss: 0.2097\n",
      "Epoch [357/500], Loss: 0.2010\n",
      "Epoch [358/500], Loss: 0.2051\n",
      "Epoch [359/500], Loss: 0.1965\n",
      "Epoch [360/500], Loss: 0.1975\n",
      "Epoch [361/500], Loss: 0.1986\n",
      "Epoch [362/500], Loss: 0.2084\n",
      "Epoch [363/500], Loss: 0.2155\n",
      "Epoch [364/500], Loss: 0.2019\n",
      "Epoch [365/500], Loss: 0.2041\n",
      "Epoch [366/500], Loss: 0.1951\n",
      "Epoch [367/500], Loss: 0.1923\n",
      "Epoch [368/500], Loss: 0.1929\n",
      "Epoch [369/500], Loss: 0.1953\n",
      "Epoch [370/500], Loss: 0.1985\n",
      "Epoch [371/500], Loss: 0.2092\n",
      "Epoch [372/500], Loss: 0.2013\n",
      "Epoch [373/500], Loss: 0.2034\n",
      "Epoch [374/500], Loss: 0.1974\n",
      "Epoch [375/500], Loss: 0.1942\n",
      "Epoch [376/500], Loss: 0.2019\n",
      "Epoch [377/500], Loss: 0.2052\n",
      "Epoch [378/500], Loss: 0.2074\n",
      "Epoch [379/500], Loss: 0.2123\n",
      "Epoch [380/500], Loss: 0.2071\n",
      "Epoch [381/500], Loss: 0.1981\n",
      "Epoch [382/500], Loss: 0.1927\n",
      "Epoch [383/500], Loss: 0.1920\n",
      "Epoch [384/500], Loss: 0.1887\n",
      "Epoch [385/500], Loss: 0.1940\n",
      "Epoch [386/500], Loss: 0.1881\n",
      "Epoch [387/500], Loss: 0.1906\n",
      "Epoch [388/500], Loss: 0.1861\n",
      "Epoch [389/500], Loss: 0.1908\n",
      "Epoch [390/500], Loss: 0.2039\n",
      "Epoch [391/500], Loss: 0.2256\n",
      "Epoch [392/500], Loss: 0.2177\n",
      "Epoch [393/500], Loss: 0.2063\n",
      "Epoch [394/500], Loss: 0.2010\n",
      "Epoch [395/500], Loss: 0.1880\n",
      "Epoch [396/500], Loss: 0.2163\n",
      "Epoch [397/500], Loss: 0.2071\n",
      "Epoch [398/500], Loss: 0.1944\n",
      "Epoch [399/500], Loss: 0.1882\n",
      "Epoch [400/500], Loss: 0.1901\n",
      "Epoch [401/500], Loss: 0.1936\n",
      "Epoch [402/500], Loss: 0.1868\n",
      "Epoch [403/500], Loss: 0.1869\n",
      "Epoch [404/500], Loss: 0.1900\n",
      "Epoch [405/500], Loss: 0.2102\n",
      "Epoch [406/500], Loss: 0.2069\n",
      "Epoch [407/500], Loss: 0.1920\n",
      "Epoch [408/500], Loss: 0.1847\n",
      "Epoch [409/500], Loss: 0.1880\n",
      "Epoch [410/500], Loss: 0.1886\n",
      "Epoch [411/500], Loss: 0.1857\n",
      "Epoch [412/500], Loss: 0.1836\n",
      "Epoch [413/500], Loss: 0.1822\n",
      "Epoch [414/500], Loss: 0.2000\n",
      "Epoch [415/500], Loss: 0.1942\n",
      "Epoch [416/500], Loss: 0.1846\n",
      "Epoch [417/500], Loss: 0.1820\n",
      "Epoch [418/500], Loss: 0.1836\n",
      "Epoch [419/500], Loss: 0.1897\n",
      "Epoch [420/500], Loss: 0.1797\n",
      "Epoch [421/500], Loss: 0.1777\n",
      "Epoch [422/500], Loss: 0.1767\n",
      "Epoch [423/500], Loss: 0.1774\n",
      "Epoch [424/500], Loss: 0.1777\n",
      "Epoch [425/500], Loss: 0.1880\n",
      "Epoch [426/500], Loss: 0.1856\n",
      "Epoch [427/500], Loss: 0.1818\n",
      "Epoch [428/500], Loss: 0.1804\n",
      "Epoch [429/500], Loss: 0.1818\n",
      "Epoch [430/500], Loss: 0.1831\n",
      "Epoch [431/500], Loss: 0.1795\n",
      "Epoch [432/500], Loss: 0.1837\n",
      "Epoch [433/500], Loss: 0.1766\n",
      "Epoch [434/500], Loss: 0.1794\n",
      "Epoch [435/500], Loss: 0.1767\n",
      "Epoch [436/500], Loss: 0.1860\n",
      "Epoch [437/500], Loss: 0.2035\n",
      "Epoch [438/500], Loss: 0.1925\n",
      "Epoch [439/500], Loss: 0.1959\n",
      "Epoch [440/500], Loss: 0.1816\n",
      "Epoch [441/500], Loss: 0.1842\n",
      "Epoch [442/500], Loss: 0.1749\n",
      "Epoch [443/500], Loss: 0.1778\n",
      "Epoch [444/500], Loss: 0.1807\n",
      "Epoch [445/500], Loss: 0.1773\n",
      "Epoch [446/500], Loss: 0.1757\n",
      "Epoch [447/500], Loss: 0.1802\n",
      "Epoch [448/500], Loss: 0.1766\n",
      "Epoch [449/500], Loss: 0.1760\n",
      "Epoch [450/500], Loss: 0.1719\n",
      "Epoch [451/500], Loss: 0.2012\n",
      "Epoch [452/500], Loss: 0.1950\n",
      "Epoch [453/500], Loss: 0.1769\n",
      "Epoch [454/500], Loss: 0.1757\n",
      "Epoch [455/500], Loss: 0.1727\n",
      "Epoch [456/500], Loss: 0.1736\n",
      "Epoch [457/500], Loss: 0.1722\n",
      "Epoch [458/500], Loss: 0.1738\n",
      "Epoch [459/500], Loss: 0.1764\n",
      "Epoch [460/500], Loss: 0.1780\n",
      "Epoch [461/500], Loss: 0.1734\n",
      "Epoch [462/500], Loss: 0.1722\n",
      "Epoch [463/500], Loss: 0.1774\n",
      "Epoch [464/500], Loss: 0.1834\n",
      "Epoch [465/500], Loss: 0.1750\n",
      "Epoch [466/500], Loss: 0.1997\n",
      "Epoch [467/500], Loss: 0.1835\n",
      "Epoch [468/500], Loss: 0.1759\n",
      "Epoch [469/500], Loss: 0.1754\n",
      "Epoch [470/500], Loss: 0.1710\n",
      "Epoch [471/500], Loss: 0.1777\n",
      "Epoch [472/500], Loss: 0.1933\n",
      "Epoch [473/500], Loss: 0.1871\n",
      "Epoch [474/500], Loss: 0.1729\n",
      "Epoch [475/500], Loss: 0.1723\n",
      "Epoch [476/500], Loss: 0.1694\n",
      "Epoch [477/500], Loss: 0.1726\n",
      "Epoch [478/500], Loss: 0.1851\n",
      "Epoch [479/500], Loss: 0.1682\n",
      "Epoch [480/500], Loss: 0.1689\n",
      "Epoch [481/500], Loss: 0.1710\n",
      "Epoch [482/500], Loss: 0.1672\n",
      "Epoch [483/500], Loss: 0.1665\n",
      "Epoch [484/500], Loss: 0.1714\n",
      "Epoch [485/500], Loss: 0.1658\n",
      "Epoch [486/500], Loss: 0.1685\n",
      "Epoch [487/500], Loss: 0.1675\n",
      "Epoch [488/500], Loss: 0.1708\n",
      "Epoch [489/500], Loss: 0.1660\n",
      "Epoch [490/500], Loss: 0.1650\n",
      "Epoch [491/500], Loss: 0.1663\n",
      "Epoch [492/500], Loss: 0.1712\n",
      "Epoch [493/500], Loss: 0.1779\n",
      "Epoch [494/500], Loss: 0.1883\n",
      "Epoch [495/500], Loss: 0.1695\n",
      "Epoch [496/500], Loss: 0.1685\n",
      "Epoch [497/500], Loss: 0.1912\n",
      "Epoch [498/500], Loss: 0.1750\n",
      "Epoch [499/500], Loss: 0.1644\n",
      "Epoch [500/500], Loss: 0.1719\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 500  # Set your desired number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels, _, _ = data  # Unpack your data tuple\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero out gradients from previous step\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        # Reshape labels to match the model output shape\n",
    "        loss = criterion(outputs, labels.squeeze(1))  # Assuming labels are in shape (batch_size, height, width)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[1;32m     14\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m data  \u001b[38;5;66;03m# Unpack only images and labels\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load test data\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)  # Adjust batch_size as needed\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "total_loss = 0.0\n",
    "total_correct = 0\n",
    "total_pixels = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for i, data in enumerate(test_loader):\n",
    "        images, labels = data  # Unpack only images and labels\n",
    "        images, labels = images.to(device), labels.long().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Upsample outputs if necessary\n",
    "        outputs = torch.nn.functional.interpolate(outputs, size=(1024, 1024), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        total_correct += correct\n",
    "        total_pixels += labels.numel()\n",
    "\n",
    "# Calculate average loss and accuracy\n",
    "average_loss = total_loss / len(test_loader)\n",
    "accuracy = total_correct / total_pixels\n",
    "\n",
    "print(f\"Average Loss: {average_loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.where(labels == 255, torch.tensor(1, device=labels.device), labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculation for testing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:  \u001b[38;5;66;03m# Use your test_loader here\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "total_loss = 0.0\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    for images, labels in test_loader:  # Use your test_loader here\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Upsample the outputs to match the label dimensions\n",
    "        outputs = nn.functional.interpolate(outputs, size=(1024, 1024), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Compute loss (same as training)\n",
    "        loss = criterion(outputs, labels.squeeze(1))  # Adjust labels if needed\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Optionally, compute accuracy, IoU, Dice Score, etc.\n",
    "\n",
    "# Print the total test loss\n",
    "print(f'Test Loss: {total_loss / len(test_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the **model****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.labels_path = labels_path\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(images_path))  # List of image files\n",
    "        self.labels = sorted(os.listdir(labels_path))  # List of label files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = np.load(os.path.join(self.images_path, self.images[idx]))\n",
    "        label = np.load(os.path.join(self.labels_path, self.labels[idx]))\n",
    "\n",
    "        # Convert image and label to float32\n",
    "        image = image.astype(np.float32)\n",
    "        label = label.astype(np.long)  # Change this line to convert to Long\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test dataset loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set paths for test dataset\n",
    "test_images_path = '/home/hzhb/Karry/data/眼底npy_test/imgs'\n",
    "test_labels_path = '/home/hzhb/Karry/data/眼底npy_test/gts'\n",
    "\n",
    "# Define transformations (if needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = MyDataset(images_path=test_images_path, labels_path=test_labels_path, transform=transform)\n",
    "\n",
    "# Create DataLoader for test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7446/3226271907.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/hzhb/Karry/my_trained_model.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FCN(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FCN(num_classes=2).to(device)\n",
    "model.load_state_dict(torch.load('/home/hzhb/Karry/my_trained_model.pth', map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FCN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mFCN\u001b[49m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/hzhb/Karry/my_trained_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FCN' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the model\n",
    "model = FCN(num_classes=2).to(device)\n",
    "model.load_state_dict(torch.load('/home/hzhb/Karry/my_trained_model.pth', map_location=device, weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Evaluation metrics\n",
    "total_pixels = 0\n",
    "correct_pixels = 0\n",
    "\n",
    "# Load test data\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device).float()\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        outputs = F.interpolate(outputs, size=(1024, 1024), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Get predicted class\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct_pixels += (preds == labels).sum().item()\n",
    "        total_pixels += labels.numel()\n",
    "\n",
    "# Final accuracy\n",
    "accuracy = correct_pixels / total_pixels\n",
    "print(f' accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of loss values must match the number of epochs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make sure the number of loss values matches the number of epochs\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(loss_values) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(epochs):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of loss values must match the number of epochs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Plotting the loss\u001b[39;00m\n\u001b[1;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: The number of loss values must match the number of epochs."
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming you have the following data from your training loop\n",
    "# epochs = list(range(1, 501))  # Epochs from 1 to 500\n",
    "# loss_values = [ \n",
    "#     0.1710, 0.1703, 0.1749, 0.1716, 0.1749, 0.1712, 0.1707,\n",
    "#     0.1657, 0.1654, 0.1663, 0.1651, 0.1691, 0.1672, 0.1670,\n",
    "#     0.1676, 0.1712, 0.1733, 0.1843, 0.1736, 0.1734, 0.1679,\n",
    "#     0.1655, 0.1664, 0.1766, 0.1676,\n",
    "#     0.1417, 0.1560, 0.1605, 0.1504,\n",
    "# ] + [0.1] * 500  # Replace 0.1 with actual loss values\n",
    "\n",
    "\n",
    "# # Make sure the number of loss values matches the number of epochs\n",
    "# if len(loss_values) != len(epochs):\n",
    "#     raise ValueError(\"The number of loss values must match the number of epochs.\")\n",
    "\n",
    "# # Plotting the loss\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(epochs[:len(loss_values)], loss_values, label='Loss', color='blue')  # Adjust epochs to match loss values\n",
    "# plt.title('Training Loss over Epochs')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
